import streamlit as st
import os
import sys
import traceback
from datetime import datetime
from typing import Any, Dict, List, Union, Optional

from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.outputs import LLMResult

# 1. è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Opentrons AI åè®®ç”Ÿæˆå™¨",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': """# Opentrons AI åè®®ç”Ÿæˆå™¨
ä½¿ç”¨AIç”Ÿæˆå’ŒéªŒè¯Opentronså®éªŒåè®®"""
    }
)

# åŸºç¡€æ ·å¼
st.markdown("""
<style>
    .main {
        background-color: #F5F7FA;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        font-weight: 500;
        border-radius: 6px;
        padding: 0.5rem 1rem;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        background-color: #388E3C;
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    
    /* ç»¿è‰²ä¸»è¦æŒ‰é’® */
    button[data-testid="chat-input-submit-button"] {
        background-color: #4CAF50 !important;
    }
    button[data-testid="chat-input-submit-button"]:hover {
        background-color: #388E3C !important;
    }
    
    .success {
        background-color: #E8F5E9;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #4CAF50;
        margin-bottom: 20px;
    }
    
    .error {
        background-color: #FFEBEE;
        padding: 15px;
        border-radius: 8px;
        border-left: 5px solid #F44336;
        color: #C62828;
        margin-bottom: 20px;
    }
    
    .info-card {
        background-color: #E3F2FD;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 20px;
    }
    
    code {
        font-size: 14px !important;
    }
</style>
""", unsafe_allow_html=True)

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from langchain_agent import agent_executor
    from opentrons_utils import run_opentrons_simulation
    has_langchain = True
except ImportError:
    st.error("æ— æ³•å¯¼å…¥LangChainä»£ç†æˆ–å…¶ä¾èµ–ã€‚åº”ç”¨å°†åœ¨æ¼”ç¤ºæ¨¡å¼ä¸‹è¿è¡Œã€‚è¯·æ£€æŸ¥æ˜¯å¦å·²å®‰è£… `langchain_openai` ç­‰åŒ…ã€‚")
    has_langchain = False
    
    def run_opentrons_simulation(protocol_code):
        return """--- æ¨¡æ‹Ÿæ¨¡å¼ ---
æ¨¡æ‹ŸéªŒè¯åè®®...
--- Result: Simulation SUCCEEDED ---"""
    
    class MockAgent:
        def invoke(self, data, config=None):
            return {"output": """from opentrons import protocol_api

requirements = {"robotType": "Flex", "apiLevel": "2.20"}

def run(protocol: protocol_api.ProtocolContext):
    # åŠ è½½ç§»æ¶²å™¨å¸å¤´æ¶åˆ°D3ä½ç½®
    tiprack = protocol.load_labware("opentrons_flex_96_tiprack_1000ul", "D3")
    
    # åŠ è½½å·¦ä¾§ç§»æ¶²å™¨
    pipette = protocol.load_instrument(
        instrument_name="flex_1channel_1000",
        mount="left",
        tip_racks=[tiprack]
    )
    
    # åŠ è½½æºæ¿åˆ°D1ä½ç½®
    source_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "D1", 
                                        label="source_plate")
    
    # åŠ è½½ç›®æ ‡æ¿åˆ°D2ä½ç½®
    destination_plate = protocol.load_labware("corning_96_wellplate_360ul_flat", "D2",
                                             label="destination_plate")
    
    # æ‰§è¡Œç§»æ¶²æ“ä½œ
    pipette.pick_up_tip()
    pipette.aspirate(50, source_plate["A1"])
    pipette.dispense(50, destination_plate["B2"])
    pipette.drop_tip()
"""}
    
    agent_executor = MockAgent()

# Callback Handler for progress display
class SimpleStreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, status_container, task_name: str = "é€šç”¨ä»»åŠ¡") -> None:
        super().__init__()
        self.status_container = status_container
        self.task_name = task_name
        self._current_tool_step = 0
        self._current_tool_name: Optional[str] = None

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        prompt_preview = prompts[0][:120] + "..." if len(prompts[0]) > 120 else prompts[0]
        self.status_container.update(
            label=f"ğŸ§  [{self.task_name}] AI æ­£åœ¨æ€è€ƒ...\næç¤ºç‰‡æ®µ: \"{prompt_preview}\"",
            state="running",
            expanded=True
        )

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        pass # Not displaying token by token for now

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        # If an agent is still running, LLM end doesn't mean the task is complete.
        self.status_container.update(
            label=f"âœ… [{self.task_name}] AI æ€è€ƒå®Œæ¯•ï¼Œå‡†å¤‡åç»­æ­¥éª¤...",
            state="running", 
            expanded=True
        )

    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
        self.status_container.update(
            label=f"âš ï¸ [{self.task_name}] AI æ€è€ƒå‡ºé”™: {error}", 
            state="error"
        )

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        # This can be too generic if agent has multiple chains.
        pass

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        pass

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        self._current_tool_step += 1
        self._current_tool_name = serialized.get("name", "æœªçŸ¥å·¥å…·")
        tool_name_display = self._current_tool_name.replace("_", " ").title()
        # Truncate input_str if it's too long
        display_input = input_str
        if len(display_input) > 200:
            display_input = display_input[:200] + "..."
        self.status_container.update(
            label=f"ğŸ› ï¸ [{self.task_name}] æ­¥éª¤ {self._current_tool_step}: ä½¿ç”¨å·¥å…· `{tool_name_display}`\nè¾“å…¥ (ç‰‡æ®µ): {display_input}",
            state="running"
        )

    def on_tool_end(
        self,
        output: str,
        **kwargs: Any,
    ) -> None:
        tool_name_display = self._current_tool_name.replace("_", " ").title() if self._current_tool_name else "å·¥å…·"
        # Truncate output if it's too long
        display_output = output
        if len(display_output) > 150:
            display_output = display_output[:150] + "..."
        self.status_container.update(
            label=f"âœ”ï¸ [{self.task_name}] å·¥å…· `{tool_name_display}` ä½¿ç”¨å®Œæ¯•ã€‚\nè¾“å‡º (ç‰‡æ®µ): {display_output}",
            state="running"
        )

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        tool_name_display = self._current_tool_name.replace("_", " ").title() if self._current_tool_name else "å·¥å…·"
        self.status_container.update(
            label=f"âš ï¸ [{self.task_name}] å·¥å…· `{tool_name_display}` ä½¿ç”¨å‡ºé”™: {error}",
            state="error"
        )

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        tool_name_display = action.tool.replace("_", " ").title()
        self.status_container.update(
            label=f"â¡ï¸ [{self.task_name}] AI è®¡åˆ’: ä½¿ç”¨å·¥å…· `{tool_name_display}`",
            state="running"
        )

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        self.status_container.update(
            label=f"ğŸ‰ [{self.task_name}] æ‰€æœ‰æ­¥éª¤å·²é¡ºåˆ©å®Œæˆ!",
            state="complete",
            expanded=False
        )
        self._current_tool_step = 0
        self._current_tool_name = None

def save_protocol_to_file(protocol_code, robot_type, api_level):
    """ä¿å­˜ç”Ÿæˆçš„åè®®åˆ°æ–‡ä»¶"""
    os.makedirs("generated_protocols", exist_ok=True)
    now_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"generated_protocols/protocol_{robot_type.lower()}_api{api_level.replace('.', '_')}_{now_timestamp}.py"
    
    with open(filename, 'w', encoding='utf-8') as pf:
        pf.write(protocol_code)
    
    return os.path.abspath(filename)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'history' not in st.session_state:
    st.session_state.history = []
if 'generated_code' not in st.session_state:
    st.session_state.generated_code = None
if 'simulation_result' not in st.session_state:
    st.session_state.simulation_result = None
if 'file_path' not in st.session_state:
    st.session_state.file_path = None
if 'is_successful' not in st.session_state:
    st.session_state.is_successful = None
if 'chat_messages' not in st.session_state:
    st.session_state.chat_messages = []

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.title("ğŸ§ª Opentrons AI")
    st.caption("è‡ªåŠ¨åŒ–åè®®ç”Ÿæˆå™¨")
    
    if not has_langchain:
        st.warning("âš ï¸ æ¼”ç¤ºæ¨¡å¼ï¼šAIåŠŸèƒ½å—é™ã€‚è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…ä»¥å¯ç”¨å®Œæ•´åŠŸèƒ½ã€‚")
    
    try:
        st.image("https://opentrons.com/wp-content/uploads/2023/07/Logo.svg", width=200)
    except:
        pass
    
    st.subheader("âš™ï¸ è®¾å¤‡è®¾ç½®")
    
    # æœºå™¨äººç±»å‹é€‰æ‹©
    if 'robot_type' not in st.session_state:
        st.session_state.robot_type = "Flex"
    
    robot_type = st.radio(
        "é€‰æ‹©æœºå™¨äººç±»å‹",
        options=["Flex", "OT-2"],
        index=0 if st.session_state.robot_type == "Flex" else 1,
        format_func=lambda x: f"ğŸ¦¾ Flex - æ–°ä¸€ä»£çµæ´»å‹æœºå™¨äºº" if x == "Flex" else f"ğŸ”¬ OT-2 - ç»å…¸å‹è‡ªåŠ¨åŒ–å¹³å°",
        key="robot_type_radio"
    )
    
    st.session_state.robot_type = robot_type
    
    # APIç‰ˆæœ¬é€‰æ‹©
    default_api_index = 6 if robot_type == "Flex" else 2  # 2.20 for Flex, 2.16 for OT-2
    api_level = st.selectbox(
        "APIç‰ˆæœ¬",
        options=["2.14", "2.15", "2.16", "2.17", "2.18", "2.19", "2.20", "2.21"],
        index=default_api_index,
        help="é€‰æ‹©Opentrons APIç‰ˆæœ¬"
    )
    
    # å†å²åè®®
    st.header("ğŸ“œ å†å²ç”Ÿæˆçš„åè®®")
    if os.path.exists("generated_protocols"):
        protocol_files = sorted([f for f in os.listdir("generated_protocols") if f.startswith("protocol_") and f.endswith(".py")], reverse=True)
        if protocol_files:
            for file_idx, file_name in enumerate(protocol_files[:5]):  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ª
                parts = file_name.split('_')
                robot_info = parts[1] if len(parts) > 1 else ""
                api_info = parts[2].replace('api', '') if len(parts) > 2 else ""
                timestamp_parts = parts[-1].split('.')[0] if len(parts) > 3 else ""
                
                robot_icon = "ğŸ¦¾" if "flex" in robot_info.lower() else "ğŸ”¬"
                button_label = f"{robot_icon} {robot_info.upper()} {api_info}"
                
                if st.button(button_label, key=f"history_btn_{file_idx}"):
                    file_path = os.path.join("generated_protocols", file_name)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            st.session_state.generated_code = f.read()
                            st.session_state.file_path = file_path
                        st.session_state.is_successful = True 
                        st.session_state.simulation_result = "ä»å†å²è®°å½•åŠ è½½ã€‚"
                        st.success(f"âœ… å·²åŠ è½½å†å²åè®®: {file_name}")
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"âŒ åŠ è½½å†å²åè®®æ—¶å‡ºé”™: {str(e)}")
        else:
            st.info("ğŸ“­ æš‚æ— å†å²åè®®æ–‡ä»¶")
    else:
        st.info("ğŸ“ å†å²è®°å½•ç›®å½•ä¸å­˜åœ¨ï¼Œå°†åœ¨ç”Ÿæˆç¬¬ä¸€ä¸ªåè®®æ—¶åˆ›å»º")

# ä¸»ç•Œé¢
st.title("ğŸ§ª Opentrons AI åè®®ç”Ÿæˆå™¨")

# è®¾å¤‡ä¿¡æ¯æç¤º
robot_emoji = "ğŸ¦¾" if robot_type == "Flex" else "ğŸ”¬"
st.info(f"{robot_emoji} å½“å‰è®¾å¤‡: {robot_type} æœºå™¨äºº | APIç‰ˆæœ¬: {api_level}")

# æ˜¾ç¤ºèŠå¤©å†å²
if st.session_state.chat_messages:
    for message in st.session_state.chat_messages:
        with st.chat_message(name=message["role"], avatar="ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"):
        content = message.get("content", "")
            if message["role"] == "assistant" and isinstance(content, str) and (
            content.startswith("from opentrons import") or 
            "def run(" in content or 
            "protocol_api" in content
        ):
                st.code(content, language="python")
        else:
            st.markdown(content)
else:
    st.markdown("ğŸ‘‹ è¿˜æ²¡æœ‰å¯¹è¯è®°å½•ï¼è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„åè®®éœ€æ±‚ã€‚")

# èŠå¤©è¾“å…¥
prompt = st.chat_input("è¯·åœ¨æ­¤å¤„è¾“å…¥æ‚¨çš„åè®®éœ€æ±‚...")

if prompt:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.chat_messages.append({
        "role": "user", 
        "content": prompt
    })
    
    # æ„å»ºè¯·æ±‚
    full_request = f"Generate an Opentrons protocol for a {robot_type} robot using API version {api_level}. {prompt}"
    
    # é‡ç½®çŠ¶æ€
    st.session_state.generated_code = None
    st.session_state.simulation_result = None
    st.session_state.file_path = None
    st.session_state.is_successful = None
    
    with st.status("ğŸ¤– AI æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...", expanded=True) as status_ui_element:
        try:
            callback_handler = SimpleStreamlitCallbackHandler(
                status_container=status_ui_element,
                task_name="åè®®ç”Ÿæˆ"
            )
            response = agent_executor.invoke(
                {"input": full_request},
                config={"callbacks": [callback_handler]}
            )
            protocol_code = response.get('output', '')
            simulation_result = run_opentrons_simulation(protocol_code)
            
            # æ›´æ–°æˆåŠŸæ¡ä»¶ï¼ŒåŒ…æ‹¬"Simulation likely SUCCEEDED"
            if "Simulation SUCCEEDED" in simulation_result or "Simulation likely SUCCEEDED" in simulation_result:
                is_successful = True
                file_path = save_protocol_to_file(protocol_code, robot_type, api_level)
            else:
                is_successful = False
                file_path = None
            
            st.session_state.generated_code = protocol_code
            st.session_state.simulation_result = simulation_result
            st.session_state.file_path = file_path
            st.session_state.is_successful = is_successful
            
            # æ·»åŠ AIå›å¤
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": protocol_code
            })
            
            # å¦‚æœæ¨¡æ‹Ÿå¤±è´¥ï¼Œè¿›è¡Œé”™è¯¯åˆ†æ
            if not is_successful and has_langchain:
                # æç¤ºé”™è¯¯åˆ†ææ­£åœ¨è¿›è¡Œ
                status_ui_element.update(
                    label="ğŸ§ AI æ­£åœ¨åˆ†æé”™è¯¯åŸå› å¹¶æä¾›å»ºè®®...", 
                    state="running", 
                    expanded=True
                )
                
                # æ„é€ ç”¨äºé”™è¯¯åˆ†æçš„æç¤º
                analysis_prompt_text = f"""ä½œä¸ºOpentronsåè®®è°ƒè¯•ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹å¤±è´¥çš„åè®®ç”Ÿæˆå°è¯•ã€‚
ç”¨æˆ·åŸå§‹è¯·æ±‚: "{prompt}"
æœºå™¨äººç±»å‹: {robot_type}
APIç‰ˆæœ¬: {api_level}
å·²ç”Ÿæˆçš„ä»£ç :
```python
{protocol_code if protocol_code else "é”™è¯¯ï¼šæœªèƒ½ç”Ÿæˆä»»ä½•ä»£ç ã€‚"}
```
æ¨¡æ‹Ÿå™¨è¿”å›çš„é”™è¯¯/ç»“æœ:
```
{simulation_result}
```
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
1. **æ ¹æœ¬åŸå› åˆ†æ**ï¼šæ¸…æ™°åœ°æŒ‡å‡ºå¯¼è‡´æ¨¡æ‹Ÿå¤±è´¥æˆ–ä»£ç é—®é¢˜çš„æœ€å¯èƒ½çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ ¹æœ¬åŸå› ã€‚
2. **å…·ä½“ä¿®æ”¹å»ºè®®**ï¼š
   * å¦‚æœé—®é¢˜åœ¨äºç”¨æˆ·åŸå§‹è¯·æ±‚çš„æ­§ä¹‰æˆ–ä¸è¶³ï¼Œè¯·æä¾›å¦‚ä½•æ”¹è¿›ç”¨æˆ·è¯·æ±‚çš„å…·ä½“ä¾‹å­ã€‚
   * å¦‚æœé—®é¢˜åœ¨äºç”Ÿæˆçš„ä»£ç ï¼Œè¯·æŒ‡å‡ºä»£ç ä¸­çš„å…·ä½“é—®é¢˜è¡Œï¼ˆå¦‚æœå¯èƒ½ï¼‰æˆ–é—®é¢˜é€»è¾‘ï¼Œå¹¶è§£é‡Šå¦‚ä½•ä¿®æ­£ã€‚
3. **è§£é‡Š**ï¼šç®€è¦è§£é‡Šä½ çš„åˆ†æè¿‡ç¨‹å’Œä½ ä¸ºä»€ä¹ˆä¼šç»™å‡ºè¿™äº›å»ºè®®ã€‚
è¯·ä»¥å‹å¥½ã€ä¹äºåŠ©äººçš„åŠ©æ‰‹å£å»ï¼Œç›´æ¥æä¾›ä¸Šè¿°åˆ†æå’Œå»ºè®®ã€‚é¿å…ä¸å¿…è¦çš„å¯’æš„ã€‚
"""
                try:
                    # ä¸ºé”™è¯¯åˆ†æä»»åŠ¡å®ä¾‹åŒ–æ–°çš„å›è°ƒå¤„ç†å™¨
                    error_analysis_callback = SimpleStreamlitCallbackHandler(
                        status_container=status_ui_element,
                        task_name="é”™è¯¯åˆ†æ"
                    )
                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    analysis_response = agent_executor.invoke(
                        {"input": analysis_prompt_text},
                        config={"callbacks": [error_analysis_callback]}
                    )
                    analysis_output = analysis_response.get('output', 'æŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ— æ³•å¯¹æ­¤é”™è¯¯è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯·æ£€æŸ¥æ¨¡æ‹Ÿå™¨æ—¥å¿—ã€‚')

                    # å°†åˆ†æç»“æœæ·»åŠ åˆ°èŠå¤©è®°å½•
                    st.session_state.chat_messages.append({
                        "role": "assistant",
                        "content": f"âŒ **æ¨¡æ‹ŸéªŒè¯æœªé€šè¿‡åˆ†æ**\n\n{analysis_output}"
                    })
                    status_ui_element.update(
                        label="ğŸ’¡ AI å·²æä¾›é”™è¯¯åˆ†æå’Œæ”¹è¿›å»ºè®®ã€‚", 
                        state="complete", 
                        expanded=False
                    )
                    
                except Exception as analysis_exc:
                    st.error(f"âš ï¸ é”™è¯¯åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–ï¼š{str(analysis_exc)}")
                    st.session_state.chat_messages.append({
                        "role": "assistant",
                        "content": f"âš ï¸ å°è¯•åˆ†æé”™è¯¯æ—¶å‡ºç°å†…éƒ¨é—®é¢˜: {str(analysis_exc)}"
                    })
                    status_ui_element.update(
                        label="âš ï¸ é”™è¯¯åˆ†æå¤±è´¥ã€‚", 
                        state="error", 
                        expanded=False
                    )
            
        except Exception as e:
            st.error(f"âš ï¸ ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            st.code(traceback.format_exc(), language="python")
            st.session_state.chat_messages.append({
                "role": "assistant",
                "content": f"âš ï¸ æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
            })

# ç»“æœå±•ç¤º
if st.session_state.generated_code is not None:
    st.markdown("---")
    st.subheader("ğŸ” ç”Ÿæˆç»“æœ")
    
    if st.session_state.is_successful:
        # ä¿®æ”¹æˆåŠŸæ¶ˆæ¯ä»¥åŒ…å«å¯èƒ½æˆåŠŸçš„æƒ…å†µ
        success_msg = "âœ… åè®®ç”ŸæˆæˆåŠŸï¼"
        if "Simulation SUCCEEDED" in st.session_state.simulation_result:
            success_msg += "æ¨¡æ‹ŸéªŒè¯å®Œå…¨é€šè¿‡ã€‚"
        elif "Simulation likely SUCCEEDED" in st.session_state.simulation_result:
            success_msg += "æ¨¡æ‹ŸéªŒè¯å¯èƒ½é€šè¿‡ï¼ˆéƒ¨åˆ†è­¦å‘Šï¼‰ã€‚"
            
        st.markdown(f"<div class='success'>{success_msg}</div>", unsafe_allow_html=True)
        if st.session_state.file_path:
            st.success(f"ğŸ“ åè®®å·²ä¿å­˜åˆ°: {st.session_state.file_path}")
            
            # ä¸‹è½½æŒ‰é’®
            with open(st.session_state.file_path, "r", encoding='utf-8') as f:
                        st.download_button(
                    "ğŸ“¥ ä¸‹è½½åè®®æ–‡ä»¶",
                    f.read(),
                    file_name=os.path.basename(st.session_state.file_path),
                    mime="text/x-python"
                )
            
            # æ˜¾ç¤ºä»£ç 
            st.code(st.session_state.generated_code, language="python")
            
            # æ˜¾ç¤ºæ¨¡æ‹Ÿç»“æœ
            with st.expander("ğŸ§ª æŸ¥çœ‹æ¨¡æ‹Ÿç»“æœ"):
                    st.text(st.session_state.simulation_result)
    else:
        st.markdown("<div class='error'>âŒ åè®®ç”Ÿæˆå¤±è´¥æˆ–æ¨¡æ‹ŸéªŒè¯æœªé€šè¿‡ã€‚</div>", unsafe_allow_html=True)
        st.error("è¯·æŸ¥çœ‹ä»¥ä¸‹ç”Ÿæˆçš„ä»£ç å’Œè¯¦ç»†æ¨¡æ‹Ÿç»“æœï¼Œæˆ–å‚è€ƒAIæä¾›çš„é”™è¯¯åˆ†æå’Œå»ºè®®ã€‚")
        
        if st.session_state.generated_code:
            with st.expander("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç ", expanded=True):
                st.code(st.session_state.generated_code, language="python")
        if st.session_state.simulation_result:
            with st.expander("âš ï¸ æŸ¥çœ‹è¯¦ç»†æ¨¡æ‹Ÿç»“æœ", expanded=True):
                st.text(st.session_state.simulation_result)

# é¦–æ¬¡ä½¿ç”¨æç¤º
if not prompt and not st.session_state.generated_code:
    st.markdown("""
    <div class='info-card'>
    <h2>ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Opentrons AI åè®®ç”Ÿæˆå™¨!</h2>
    
    è¿™ä¸ªå·¥å…·å¯ä»¥å¸®åŠ©æ‚¨å¿«é€Ÿç”Ÿæˆ Opentrons å®éªŒåè®®ã€‚åªéœ€æè¿°æ‚¨çš„å®éªŒéœ€æ±‚ï¼ŒAI å°†ç”Ÿæˆå¯æ‰§è¡Œçš„ Python ä»£ç ã€‚
    
    <h3>ğŸ’¡ æé—®ç¤ºä¾‹:</h3>
    <ul>
        <li>"ç”Ÿæˆä¸€ä¸ªæ–¹æ¡ˆï¼Œç”¨1000uLç§»æ¶²å™¨å°†A1å­”çš„50uLæ¶²ä½“è½¬ç§»åˆ°B2å­”ï¼Œä½¿ç”¨Flexæœºå™¨äººå’ŒAPI 2.20"</li>
        <li>"è®¾è®¡ä¸€ä¸ªå®éªŒï¼Œä»æºæ¿çš„A1-A6å­”å¸å–100uLæ¶²ä½“ï¼Œåˆ†åˆ«åˆ†é…åˆ°ç›®æ ‡æ¿çš„B1-B6å­”ï¼Œå¹¶ä¸ºæ¯æ¬¡è½¬ç§»æ›´æ¢å¸å¤´"</li>
        <li>"ç¼–å†™ä¸€ä¸ªOT-2åè®®ï¼ŒAPIç‰ˆæœ¬2.16ï¼Œå®ç°ä»ç¬¬ä¸€åˆ—å¼€å§‹ï¼Œåœ¨96å­”æ¿ä¸­è¿›è¡Œ1:2çš„è¿ç»­ç¨€é‡Šï¼Œå…±ç¨€é‡Š6åˆ—ï¼Œæ¯å­”æ€»ä½“ç§¯200uL"</li>
    </ul>
    
    <h3>ä½¿ç”¨æ­¥éª¤:</h3>
    1. åœ¨ä¾§è¾¹æ é€‰æ‹©æ‚¨çš„æœºå™¨äººç±»å‹å’Œ API ç‰ˆæœ¬
    2. åœ¨ä¸‹æ–¹è¾“å…¥æ¡†æè¿°æ‚¨çš„å®éªŒéœ€æ±‚
    3. AI å°†ç”Ÿæˆåè®®å¹¶è‡ªåŠ¨éªŒè¯
    4. ä¸‹è½½å¹¶ä½¿ç”¨ç”Ÿæˆçš„åè®®æ–‡ä»¶
    </div>
    """, unsafe_allow_html=True)