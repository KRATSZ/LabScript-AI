# -*- coding: utf-8 -*-
"""Main execution script for the Opentrons AI Protocol Generator."""
import os
import re
import traceback
import shutil
from datetime import datetime
import time
import colorama
from colorama import Fore, Style, Back

# Initialize colorama for cross-platform colored terminal output
colorama.init()

# Import necessary components from our modules
from langchain_agent import agent_executor
from opentrons_utils import run_opentrons_simulation # For final verification

# Utility Functions for Terminal UI
def clear_screen():
    """Clear the terminal screen."""
    os.system('cls' if os.name == 'nt' else 'clear')

def print_header():
    """Print the application header."""
    terminal_width = shutil.get_terminal_size().columns
    header = f"{Fore.CYAN}Opentrons AI Protocol Generator{Style.RESET_ALL}"
    centered_header = header.center(terminal_width)
    
    print("\n" + "=" * terminal_width)
    print(centered_header)
    print("=" * terminal_width + "\n")

def print_info(message):
    """Print info message with styling."""
    print(f"{Fore.BLUE}â„¹ï¸ INFO:{Style.RESET_ALL} {message}")

def print_success(message):
    """Print success message with styling."""
    print(f"{Fore.GREEN}âœ… SUCCESS:{Style.RESET_ALL} {message}")

def print_warning(message):
    """Print warning message with styling."""
    print(f"{Fore.YELLOW}âš ï¸ WARNING:{Style.RESET_ALL} {message}")

def print_error(message):
    """Print error message with styling."""
    print(f"{Fore.RED}âŒ ERROR:{Style.RESET_ALL} {message}")

def print_robot_info(robot_type, api_level):
    """Print current robot and API settings."""
    print(f"\n{Fore.CYAN}Current Settings:{Style.RESET_ALL}")
    print(f"  Robot Type: {Fore.MAGENTA}{robot_type}{Style.RESET_ALL}")
    print(f"  API Level: {Fore.MAGENTA}{api_level}{Style.RESET_ALL}\n")

def print_thinking_animation(message="AIæ€è€ƒä¸­", duration=2):
    """Show a thinking animation."""
    for _ in range(duration * 2):
        for dots in [".", "..", "..."]:
            print(f"\r{Fore.YELLOW}{message}{dots}{Style.RESET_ALL}", end='', flush=True)
            time.sleep(0.25)
    print("\r" + " " * (len(message) + 5), end="\r")  # Clear the line

def format_chat_history(chat_history):
    """Format the chat history for display."""
    if not chat_history:
        return f"{Fore.YELLOW}æš‚æ— å¯¹è¯å†å²{Style.RESET_ALL}"
    
    formatted_history = []
    for msg in chat_history:
        role_color = Fore.BLUE if msg["role"] == "user" else Fore.GREEN
        role_icon = "ğŸ§‘" if msg["role"] == "user" else "ğŸ¤–"
        role_name = "User" if msg["role"] == "user" else "AI"
        
        # Format content based on type
        content = msg["content"]
        if msg["role"] == "assistant" and (
            content.startswith("from opentrons import") or 
            content.startswith("metadata =") or 
            content.startswith("requirements =")
        ):
            # For code responses, shorten and indicate it's code
            lines = content.split("\n")
            if len(lines) > 5:
                content = "\n".join(lines[:5]) + f"\n{Fore.YELLOW}... [code continues, {len(lines)} lines total]{Style.RESET_ALL}"
            content = f"{Fore.CYAN}[Python Code]{Style.RESET_ALL}\n{content}"
        
        formatted_msg = f"{role_color}{role_icon} {role_name}:{Style.RESET_ALL} {content}"
        formatted_history.append(formatted_msg)
    
    return "\n\n".join(formatted_history)

# User Interaction Functions (Improved from the original script)
def get_robot_type_and_api_level():
    """Ask user to choose robot model and optionally API level."""
    clear_screen()
    print_header()
    print(f"{Fore.CYAN}Robot Selection{Style.RESET_ALL}")
    print(f"{Fore.WHITE}------------------------{Style.RESET_ALL}")
    print(f"1: {Fore.MAGENTA}Flex{Style.RESET_ALL} - æ–°ä¸€ä»£çµæ´»å‹æœºå™¨äºº [æ¨èAPI: 2.20+]")
    print(f"2: {Fore.MAGENTA}OT-2{Style.RESET_ALL} - ç»å…¸å‹è‡ªåŠ¨åŒ–å¹³å° [æ¨èAPI: 2.16-2.20]")
    print(f"{Fore.WHITE}------------------------{Style.RESET_ALL}")
    
    robot_choice_input = input(f"\né€‰æ‹©æœºå™¨äººç±»å‹ (1=Flex, 2=OT-2) [{Fore.CYAN}é»˜è®¤: 1{Style.RESET_ALL}]: ").strip()
    robot_type = "OT-2" if robot_choice_input == "2" else "Flex"
    
    default_api = "2.20" if robot_type == "Flex" else "2.16" # Best default API version for each robot
    
    if robot_type == "OT-2":
        api_recommendation = "2.16-2.20 (å»ºè®®: 2.16)"
    else: # Flex
        api_recommendation = "2.20 æˆ–æ›´é«˜ (å»ºè®®: 2.20)"

    api_level_input = input(f"\nè¾“å…¥ {robot_type} çš„APIç‰ˆæœ¬ [æ¨è: {Fore.CYAN}{api_recommendation}{Style.RESET_ALL}, é»˜è®¤: {Fore.CYAN}{default_api}{Style.RESET_ALL}]: ").strip()
    # Basic validation to check if it looks like a version number
    api_level = api_level_input if api_level_input and api_level_input.replace('.', '', 1).isdigit() else default_api
    
    print_success(f"å·²é€‰æ‹©æœºå™¨äºº: {Fore.MAGENTA}{robot_type}{Style.RESET_ALL}, APIç‰ˆæœ¬: {Fore.MAGENTA}{api_level}{Style.RESET_ALL}")
    time.sleep(1)  # Brief pause to show the selection
    return robot_type, api_level

def get_user_request(robot_type, api_level, chat_history=None):
    """Get user request for the protocol with better UI."""
    clear_screen()
    print_header()
    print_robot_info(robot_type, api_level)
    
    # Show chat history if it exists
    if chat_history and len(chat_history) > 0:
        print(f"{Fore.CYAN}å¯¹è¯å†å²:{Style.RESET_ALL}")
        print(format_chat_history(chat_history))
        print("\n" + "-" * shutil.get_terminal_size().columns + "\n")
    
    print(f"{Fore.CYAN}è¯·æ±‚è¾“å…¥{Style.RESET_ALL}")
    print(f"{Fore.WHITE}------------------------{Style.RESET_ALL}")
    
    use_default_q = input(f"ä½¿ç”¨é»˜è®¤åè®®ç¤ºä¾‹? (y/n) [{Fore.CYAN}é»˜è®¤: y{Style.RESET_ALL}]: ").strip().lower()
    
    contextual_request_prefix = f"Generate an Opentrons protocol for a {robot_type} robot using API version {api_level}. The protocol should: "

    if use_default_q != "n": 
        if robot_type == "Flex":
            return contextual_request_prefix + ("transfer 50uL of liquid from well A1 of a 'corning_96_wellplate_360ul_flat' labware (named 'source_plate') on deck slot D1 "
                                                "to well B2 of another 'corning_96_wellplate_360ul_flat' labware (named 'destination_plate') on deck slot D2. "
                                                "Use a 'flex_1channel_1000' pipette mounted on the left. "
                                                "The tip rack is an 'opentrons_flex_96_tiprack_1000ul' located on deck slot D3. "
                                                "Load a trash bin in deck slot A3.")
        else:  # OT-2
            return contextual_request_prefix + ("transfer 50uL of liquid from well A1 of a 'corning_96_wellplate_360ul_flat' labware (named 'source_plate') on deck slot 1 "
                                                "to well B2 of another 'corning_96_wellplate_360ul_flat' labware (named 'destination_plate') on deck slot 2. "
                                                "Use a 'p300_single_gen2' pipette mounted on the left. "
                                                "The tip rack is an 'opentrons_96_tiprack_300ul' located on deck slot 3. "
                                                "The OT-2 has a fixed trash in slot 12, so no need to load it explicitly for basic transfers.")
    else:
        print(f"\n{Fore.CYAN}è¯·æè¿°æ‚¨çš„è‡ªå®šä¹‰åè®®. å…·ä½“è¯´æ˜:{Style.RESET_ALL}")
        print(f"- {robot_type} æœºå™¨äººä¸Šçš„åŠ¨ä½œ (è½¬ç§»æ¶²ä½“, æ··åˆ, åŠ è½½æ¨¡å—ç­‰)")
        print("- å®éªŒææ–™: å‡†ç¡®çš„APIåç§°, è‡ªå®šä¹‰åç§°, å¹³å°ä½ç½®")
        print("- ç§»æ¶²å™¨: å‡†ç¡®çš„APIåç§°å’ŒåŠ è½½ä½ç½® (å·¦/å³)")
        print("- ä½“ç§¯, å¸å¤´å¤„ç†æ–¹å¼ (æ–°å¸å¤´ï¼Œé‡å¤ä½¿ç”¨), æ··åˆå‚æ•°ç­‰\n")
        
        if chat_history and len(chat_history) > 0:
            print(f"{Fore.YELLOW}ğŸ’¡ æç¤º: æ‚¨å¯ä»¥å¼•ç”¨ä¹‹å‰çš„å¯¹è¯æˆ–è¦æ±‚ä¿®æ”¹å·²ç”Ÿæˆçš„åè®®{Style.RESET_ALL}")
        
        custom_request_description = input(f"\næ‚¨çš„åè®®æè¿°: ").strip()
        
        if not custom_request_description:
            print_warning("æè¿°ä¸ºç©ºï¼Œå°†ä½¿ç”¨é»˜è®¤åè®®ç¤ºä¾‹ã€‚")
            return get_user_request(robot_type, api_level, chat_history)
        
        # Ensure the contextual prefix is part of the request for the agent, without duplicating if user included it.
        # Check if the user request already starts with a similar phrase
        lower_custom_request = custom_request_description.lower()
        lower_prefix_start = contextual_request_prefix.lower().split(':')[0] # e.g., "generate an opentrons protocol for a flex robot..."
        
        if not lower_custom_request.startswith(lower_prefix_start):
             print_info("æ­£åœ¨å°†æœºå™¨äººç±»å‹å’ŒAPIç‰ˆæœ¬ä¸Šä¸‹æ–‡é¢„å…ˆæ·»åŠ åˆ°è¯·æ±‚ä¸­ã€‚")
             return contextual_request_prefix + custom_request_description
        else:
             print_info("ç”¨æˆ·è¯·æ±‚ä¼¼ä¹å·²ç»åŒ…å«æœºå™¨äºº/APIä¸Šä¸‹æ–‡ã€‚")
             return custom_request_description


# Main Execution Block
if __name__ == "__main__":
    clear_screen()
    print_header()
    print_info("æ­£åœ¨åˆå§‹åŒ–...")
    
    selected_robot_type, selected_api_level = get_robot_type_and_api_level()
    
    # Initialize chat history
    chat_history = []
    generated_code = None
    
    # Multi-turn conversation loop
    while True:
        user_protocol_request = get_user_request(selected_robot_type, selected_api_level, chat_history)
        
        # Add user message to chat history
        chat_history.append({
            "role": "user",
            "content": user_protocol_request,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })
        
        clear_screen()
        print_header()
        print_robot_info(selected_robot_type, selected_api_level)
        print_info(f"æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        print_thinking_animation("AIæ­£åœ¨ç”Ÿæˆåè®®", 3)
        
        try:
            # Invoke the agent executor from langchain_agent module
            agent_response = agent_executor.invoke({"input": user_protocol_request})
            generated_code = agent_response.get('output', 'Agent did not return code in the expected format, or an error occurred.')

            # Add AI response to chat history
            chat_history.append({
                "role": "assistant",
                "content": generated_code,
                "timestamp": datetime.now().strftime("%H:%M:%S")
            })
            
            # --- Code Extraction Logic ---
            print_info("æ­£åœ¨æå–Pythonä»£ç ...")
            
            if isinstance(generated_code, str):
                markdown_match = re.search(r"```(?:python)?\n(.*?)```", generated_code, re.DOTALL | re.IGNORECASE)
                if markdown_match:
                    print_info("åœ¨AIè¾“å‡ºä¸­æ‰¾åˆ°Pythonä»£ç å—.")
                    extracted_code = markdown_match.group(1).strip()
                else:
                    code_start_indicators = ["from opentrons import", "metadata =", "requirements ="]
                    stripped_agent_output = generated_code.strip()
                    if any(stripped_agent_output.startswith(indicator) for indicator in code_start_indicators):
                        print_info("AIè¾“å‡ºçœ‹èµ·æ¥æ˜¯åŸå§‹Pythonä»£ç .")
                        extracted_code = stripped_agent_output
                    else:
                        print_info("AIè¾“å‡ºä¸æ˜¯åŸå§‹ä»£ç æˆ–Markdownæ ¼å¼. æœç´¢å…³é”®è¯...")
                        code_found_by_keyword_search = False
                        for keyword in code_start_indicators:
                            if keyword in stripped_agent_output:
                                start_idx = stripped_agent_output.find(keyword)
                                extracted_code = stripped_agent_output[start_idx:]
                                print_info(f"æ‰¾åˆ°å…³é”®è¯ '{keyword}'. æå–æ½œåœ¨ä»£ç .")
                                code_found_by_keyword_search = True
                                break
                        if not code_found_by_keyword_search:
                            print_warning("æ— æ³•å¯é åœ°æå–Pythonä»£ç å—.")
                            extracted_code = stripped_agent_output 
                generated_code = extracted_code
            else:
                print_warning(f"AIè¿”å›éå­—ç¬¦ä¸²è¾“å‡º: {type(generated_code)}. æ­£åœ¨è½¬æ¢.")
                generated_code = str(generated_code)

            # Show the generated code
            print("\n" + "=" * shutil.get_terminal_size().columns)
            print(f"{Fore.CYAN}ç”Ÿæˆçš„åè®®ä»£ç :{Style.RESET_ALL}")
            print(f"{Fore.GREEN}{generated_code}{Style.RESET_ALL}")
            print("=" * shutil.get_terminal_size().columns + "\n")

            # --- Final Verification Simulation ---
            print_info("æ­£åœ¨è¿è¡Œæœ€ç»ˆéªŒè¯æ¨¡æ‹Ÿ...")
            
            looks_like_a_protocol = isinstance(generated_code, str) and \
                                    generated_code.strip() and \
                                    "def run(" in generated_code and \
                                    ("from opentrons import protocol_api" in generated_code or "import opentrons.protocol_api" in generated_code) and \
                                    ("metadata =" in generated_code or "requirements =" in generated_code)

            if looks_like_a_protocol:
                print_info("æå–çš„ä»£ç çœ‹èµ·æ¥åˆç†. è¿è¡Œæ¨¡æ‹Ÿ...")
                
                # Show thinking animation
                print_thinking_animation("æ¨¡æ‹ŸéªŒè¯ä¸­", 2)
                
                final_sim_output = run_opentrons_simulation(generated_code) # Use the imported function
                
                # Check if simulation successful
                strict_success_indicators = [
                    "Result: Simulation SUCCEEDED",
                    "Result: Simulation likely SUCCEEDED"
                ]
                is_strictly_successful = any(indicator in final_sim_output for indicator in strict_success_indicators)
                is_successful_with_warnings = "Result: Simulation SUCCEEDED WITH UNEXPECTED STDERR" in final_sim_output or \
                                              "Result: Simulation COMPLETED WITH UNEXPECTED STDERR" in final_sim_output
                
                if is_strictly_successful or (is_successful_with_warnings and "Result: Simulation FAILED" not in final_sim_output):
                    print_success("æ¨¡æ‹ŸéªŒè¯é€šè¿‡!")
                    if is_successful_with_warnings and not is_strictly_successful:
                        print_warning("æ¨¡æ‹Ÿé€šè¿‡ä½†æœ‰æ„å¤–çš„STDERR. è¯·ä»”ç»†æ£€æŸ¥è¾“å‡º.")
                    
                    save_file_q = input(f"\nä¿å­˜åè®®åˆ°.pyæ–‡ä»¶? (y/n) [{Fore.CYAN}é»˜è®¤: y{Style.RESET_ALL}]: ").strip().lower()
                    if save_file_q != "n":
                        # Create directory if it doesn't exist
                        os.makedirs("generated_protocols", exist_ok=True)
                        
                        now_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        suggested_filename = f"generated_protocols/protocol_{selected_robot_type.lower()}_api{selected_api_level.replace('.', '_')}_{now_timestamp}.py"
                        
                        user_filename = input(f"è¾“å…¥æ–‡ä»¶å [{Fore.CYAN}é»˜è®¤: {os.path.basename(suggested_filename)}{Style.RESET_ALL}]: ").strip()
                        chosen_filename = user_filename if user_filename else suggested_filename
                        if not chosen_filename.endswith('.py'):
                            chosen_filename += '.py'
                        
                        # Ensure directory part exists
                        dirname = os.path.dirname(chosen_filename)
                        if dirname and not os.path.exists(dirname):
                            os.makedirs(dirname, exist_ok=True)
                        
                        try:
                            with open(chosen_filename, 'w', encoding='utf-8') as pf:
                                pf.write(generated_code)
                            print_success(f"åè®®å·²ä¿å­˜åˆ°: {os.path.abspath(chosen_filename)}")
                        except IOError as e_save_io:
                            print_error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e_save_io}")
                        except Exception as e_save_gen:
                            print_error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e_save_gen}")
                else:
                    print_error("æ¨¡æ‹ŸæŠ¥å‘Šäº†ä¸¥é‡é”™è¯¯æˆ–æœªæˆåŠŸ.")
                    print(f"\n{Fore.YELLOW}æ¨¡æ‹Ÿè¾“å‡ºæ‘˜è¦:{Style.RESET_ALL}")
                    
                    # Extract and show the relevant error parts
                    error_lines = [line for line in final_sim_output.split('\n') 
                                if "ERROR" in line or "Error" in line or "Traceback" in line or "FAILED" in line]
                    if error_lines:
                        for line in error_lines[:10]:  # Show first 10 error lines
                            print(f"{Fore.RED}{line}{Style.RESET_ALL}")
                        if len(error_lines) > 10:
                            print(f"{Fore.YELLOW}... è¿˜æœ‰ {len(error_lines) - 10} è¡Œé”™è¯¯ä¿¡æ¯ ...{Style.RESET_ALL}")
                    else:
                        print(f"{Fore.YELLOW}æœªæ‰¾åˆ°å…·ä½“é”™è¯¯ä¿¡æ¯. è¯·æŸ¥çœ‹å®Œæ•´æ¨¡æ‹Ÿè¾“å‡º.{Style.RESET_ALL}")
            else:
                print_error("æå–çš„AIè¾“å‡ºä¸åƒæœ‰æ•ˆçš„Opentronsåè®®.")
                print(f"{Fore.YELLOW}AIè¾“å‡ºç‰‡æ®µ (ä¾›å®¡æŸ¥):{Style.RESET_ALL}")
                print(f"{Fore.RED}{generated_code[:600]}...{Style.RESET_ALL}")

        except Exception as main_execution_error:
            print_error("AIæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
            print(f"{Fore.RED}é”™è¯¯ç±»å‹: {type(main_execution_error)}{Style.RESET_ALL}")
            print(f"{Fore.RED}é”™è¯¯è¯¦æƒ…: {main_execution_error}{Style.RESET_ALL}")
            traceback.print_exc()
        
        # Ask if the user wants to continue the conversation
        print("\n" + "-" * shutil.get_terminal_size().columns)
        continue_chat = input(f"\nç»§ç»­å¯¹è¯? (y/n) [{Fore.CYAN}é»˜è®¤: y{Style.RESET_ALL}]: ").strip().lower()
        if continue_chat == "n":
            print_info("æ„Ÿè°¢ä½¿ç”¨Opentrons AIåè®®ç”Ÿæˆå™¨!")
            break 